@misc{wu2024knowledgeinfusedlegalwisdomnavigating,
      title={Knowledge-Infused Legal Wisdom: Navigating LLM Consultation through the Lens of Diagnostics and Positive-Unlabeled Reinforcement Learning}, 
      author={Yang Wu and Chenghao Wang and Ece Gumusel and Xiaozhong Liu},
      year={2024},
      eprint={2406.03600},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.03600}, 
    }
@inproceedings{pennisi-etal-2023-nomos,
    title = "{NOMOS}: Navigating Obligation Mining in Official Statutes",
    author = "Pennisi, Andrea  and
      Gonz{\'a}lez Hern{\'a}ndez, Elvira  and
      Koivula, Nina",
    editor = "Preo{\textcommabelow{t}}iuc-Pietro, Daniel  and
      Goanta, Catalina  and
      Chalkidis, Ilias  and
      Barrett, Leslie  and
      Spanakis, Gerasimos  and
      Aletras, Nikolaos",
    booktitle = "Proceedings of the Natural Legal Language Processing Workshop 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.nllp-1.2",
    doi = "10.18653/v1/2023.nllp-1.2",
    pages = "8--16",
    abstract = "The process of identifying obligations in a legal text is not a straightforward task, because not only are the documents long, but the sentences therein are long as well. As a result of long elements in the text, law is more difficult to interpret (Coupette et al., 2021). Moreover, the identification of obligations relies not only on the clarity and precision of the language used but also on the unique perspectives, experiences, and knowledge of the reader. In particular, this paper addresses the problem of identifyingobligations using machine and deep learning approaches showing a full comparison between both methodologies and proposing a new approach called NOMOS based on the combination of Positional Embeddings (PE) and Temporal Convolutional Networks (TCNs). Quantitative and qualitative experiments, conducted on legal regulations 1, demonstrate the effectiveness of the proposed approach.",
}
@misc{chakraborty2023revolutionizing,
    title = {Revolutionizing the Legal Landscape: How AI is Transforming the Legal Industry},
    author = {Haimantika Chakraborty},
    journal = {International Journal of Law Management \& Humanities},
    volume = {6},
    year = {2023},
}

@misc{mavi2023retrievalaugmentedchainofthoughtsemistructureddomains,
      title={Retrieval-Augmented Chain-of-Thought in Semi-structured Domains}, 
      author={Vaibhav Mavi and Abulhair Saparov and Chen Zhao},
      year={2023},
      eprint={2310.14435},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.14435}, 
}

@misc{li2024experimentinglegalaisolutions,
      title={Experimenting with Legal AI Solutions: The Case of Question-Answering for Access to Justice}, 
      author={Jonathan Li and Rohan Bhambhoria and Samuel Dahan and Xiaodan Zhu},
      year={2024},
      eprint={2409.07713},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.07713}, 
}

@misc{pipitone2024legalbenchragbenchmarkretrievalaugmentedgeneration,
      title={LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the Legal Domain}, 
      author={Nicholas Pipitone and Ghita Houir Alami},
      year={2024},
      eprint={2408.10343},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.10343}, 
}

@article{dahan2023lawyers,
  title={Lawyers Should Not Trust AI: A call for an Open-source Legal Language Model},
  author={Dahan, Samuel and Bhambhoria, Rohan and Liang, David and Zhu, Xiaodan},
  journal={Queen's University Legal Research Paper},
  year={2023},
  note={Available at SSRN: https://ssrn.com/abstract=4587092 or http://dx.doi.org/10.2139/ssrn.4587092}
}

@misc{hu2025finetuninglargelanguagemodels,
      title={Fine-tuning Large Language Models for Improving Factuality in Legal Question Answering}, 
      author={Yinghao Hu and Leilei Gan and Wenyi Xiao and Kun Kuang and Fei Wu},
      year={2025},
      eprint={2501.06521},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.06521}, 
}

@inproceedings{martinez-etal-2025-scalable,
    title = "A Scalable Framework for Legal Text Understanding in Regulatory and Financial Contexts.",
    author = "Mart{\'i}nez, Santiago  and
      Casta{\~n}eda, Juan Manuel  and
      Manrique, Ruben",
    editor = "Chen, Chung-Chi  and
      Moreno-Sandoval, Antonio  and
      Huang, Jimin  and
      Xie, Qianqian  and
      Ananiadou, Sophia  and
      Chen, Hsin-Hsi",
    booktitle = "Proceedings of the Joint Workshop of the 9th Financial Technology and Natural Language Processing (FinNLP), the 6th Financial Narrative Processing (FNP), and the 1st Workshop on Large Language Models for Finance and Legal (LLMFinLegal)",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.finnlp-1.39/",
    pages = "326--334",
    abstract = "This study presents a comprehensive approach to developing a domain-specific large language model (LLM) for regulatory and financial text interpretation. A specialized corpus was constructed through large-scale scraping of financial and regulatory documents across domains such as compliance, licensing, and financial reporting. The data was preprocessed using GPT-4o-mini with prompt engineering to retain critical information and remove noise. We further pre-trained a LLaMA-3.1-8B model on the curated corpus and fine-tuned it using an instruction dataset covering nine tasks from the Coling 2025 Regulations Challenge, including acronym expansion, regulatory question-answering, and XBRL-based financial analytics, employing QLoRA to reduce memory requirements. The model exhibits a slight improvement from baseline answering complex regulatory questions (detailed QA) and expanding acronyms. This study demonstrates the potential of domain-specific LLMs in regulatory text interpretation and lays the groundwork for future research in specialized NLP evaluation methodologies."
}
