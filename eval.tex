\chapter{Evaluation}
\section{Evaluation Framework}
The evaluation of the agentic pipeline was done in collaboration with legal students and experts
\begin{table}[h]
    \centering
    \caption{Criteria Framework for Human Evaluation of Legal AI System}
    \begin{tabular}{p{3cm}p{12cm}}
    \hline
    \textbf{Criterion} & \textbf{Description} \\
    \hline
    Legal Accuracy & This metric assesses whether the legal information provided in the system's output is factually correct, consistent with Colombian law, and properly interprets relevant statutes, regulations, and precedents applicable to gender-based cases. \\
    \hline
    Completeness & This metric evaluates whether the system extracts all necessary information during the conversational phase to form a comprehensive understanding of the case, without missing critical details that would affect the legal strategy. \\
    \hline
    Relevance & This metric measures whether the legal strategies and guidance generated by the system are directly applicable to the specific circumstances of the case, address the core legal issues at hand, and provide actionable next steps tailored to the user's situation. \\
    \hline
    Clarity & This metric assesses whether the legal guidance is communicated in language that is accessible to users with limited legal literacy, avoids unnecessary jargon, explains technical concepts when needed, and presents information in a logical, easy-to-follow structure. \\
    \hline
    Conversational Appropriateness & This metric evaluates the system's ability to conduct sensitive conversations about gender-based legal issues, including appropriate phrasing of questions, recognition of emotional contexts, and maintenance of a supportive, non-judgmental tone throughout the interaction. \\
    \hline
    Procedural Correctness & This metric examines whether the system accurately identifies and explains the proper legal procedures, timelines, documentation requirements, and institutional pathways relevant to the user's case within the Colombian justice system. \\
    \hline
    Legal Utility & This metric evaluates the practical value of the system's output for users navigating gender-based legal issues, including actionability of advice, prioritization of critical information, and potential to improve user outcomes in the legal process. \\
    \hline
    \end{tabular}
\end{table}

\subsection{Evaluation Methodology}
The evaluation of our agentic pipeline for gender-based legal cases 
incorporates both automated metrics and human assessment. For automated 
evaluation, we employ two complementary approaches:
\subsubsection{BERTScore for Semantic Similarity}
We utilize BERTScore~\cite{zhang2020bertscoreevaluatingtextgeneration} to measure the semantic similarity between system-generated legal strategies and reference strategies prepared by legal experts. Unlike surface-level metrics such as BLEU or ROUGE, 
BERTScore leverages contextual embeddings to capture deeper semantic 
correspondence, making it particularly suitable for evaluating legal 
content where precise meaning is critical. For each test case, we compute 
precision, recall, and F1 scores using:
\begin{equation}
P_{BERT} = \frac{1}{|x|}\sum_{x_i \in x}\max_{y_j \in y}\cos(e(x_i), e(y_j))
\end{equation}
\begin{equation}
R_{BERT} = \frac{1}{|y|}\sum_{y_j \in y}\max_{x_i \in x}\cos(e(x_i), e(y_j))
\end{equation}
\begin{equation}
F_{BERT} = 2\frac{P_{BERT} \cdot R_{BERT}}{P_{BERT} + R_{BERT}}
\end{equation}
where $e(\cdot)$ represents contextualized embeddings from a pre-trained language model fine-tuned on Colombian legal texts.

\subsubsection{LLM-based Relevance Classification for RAG}
To evaluate the effectiveness of our RAG component, we implement a 
prompt-engineering approach for relevance classification~\cite{es2023ragasautomatedevaluationretrieval}. 
For each retrieved document chunk, we design a systematic prompt 
that instructs the LLM to assess relevance with a binary classification:

\begin{verbatim}
Given the legal case context: [CASE DESCRIPTION]
Evaluate the following document chunk for relevance:
[RETRIEVED DOCUMENT CHUNK]

Respond ONLY with 'RELEVANT' or 'NOT RELEVANT' 
based on whether this chunk directly supports 
resolving the legal issue described.
\end{verbatim}

This method leverages the zero-shot capabilities of large language 
models to perform binary classification without requiring extensive 
fine-tuning. We apply this approach across a diverse set of gender-based 
legal cases, collecting relevance assessments for multiple retrieved 
document chunks.

The evaluation metrics are computed as follows:
\begin{equation}
Precision = \frac{|\text{Retrieved Relevant Documents}|}{|\text{All Retrieved Documents}|}
\end{equation}

\begin{equation}
Recall = \frac{|\text{Retrieved Relevant Documents}|}{|\text{All Relevant Documents}|}
\end{equation}

We acknowledge the potential limitations of this approach, including 
prompt sensitivity and the need for careful prompt design. To mitigate 
these concerns we conduct human validation to cross-check the LLM's relevance assessments~\cite{es2023ragasautomatedevaluationretrieval}.

The primary objective is to quantify the retrieval component's 
effectiveness by analyzing the proportion of contextually relevant 
documents retrieved for each legal case scenario. This approach provides 
an automated method for assessing RAG performance, complementing our 
human evaluation framework.

\section{Evaluation Methodology for Agentic Components}
Our evaluation strategy recognizes the distinctive challenges posed by 
each agent within the legal guidance pipeline. The methodology balances 
quantitative precision with qualitative insight, acknowledging the nuanced 
nature of legal communication.

The information extraction agent, responsible for conversationally 
gathering case details, will be assessed through a mixed-methods approach. 
Quantitative metrics will measure the completeness and accuracy of 
information gathered, tracking the agent's ability to systematically 
elicit critical legal context. Simultaneously, leveraging the previously 
developed database, we evaluate the model’s legal strategy generation by 
measuring its proximity to the ground truth using BERTScore, assessing 
its performance in this specific task.

For the retrieval-augmented generation (RAG) component, our evaluation 
centers on semantic relevance and source credibility. The previously 
discussed BERTScore and LLM-based relevance classification will provide 
computational metrics assessing how precisely retrieved legal documents 
align with the specific case context. These automated evaluations will 
be complemented by expert reviews, where legal professionals assess the 
substantive accuracy and jurisdictional appropriateness of the retrieved 
sources.

The communication agent demands a more nuanced evaluation framework. 
Beyond traditional linguistic metrics, we will employ user experience 
surveys and expert assessments to gauge the agent's ability to translate 
complex legal strategies into accessible, actionable guidance. The goal 
is to measure not just linguistic clarity, but the agent's capacity to 
demystify legal processes for individuals with varying levels of legal 
literacy.


By integrating computational metrics with human expertise, our evaluation 
methodology seeks to move beyond traditional performance assessments. 
We aim to develop a holistic understanding of how AI can effectively 
support legal empowerment, particularly for marginalized communities 
navigating complex gender-based legal challenges.

\section{Legal Specific questions Questions}
Legal Accuracy:
¿En qué medida la información legal proporcionada por el sistema es precisa, actualizada y correctamente contextualizada dentro del marco jurídico colombiano sobre asuntos de género?

Completeness:
¿Hasta qué punto el sistema solicita y extrae toda la información relevante y necesaria durante la fase conversacional, sin omitir detalles críticos que podrían afectar la estrategia legal?

Relevance:
¿En qué grado las estrategias legales y orientaciones generadas por el sistema responden directamente a las circunstancias específicas del caso y proporcionan pasos accionables adaptados a la situación particular?

Clarity:
¿Con qué eficacia el sistema comunica información legal compleja en un lenguaje accesible, evitando jerga innecesaria y explicando conceptos técnicos de manera que resulten comprensibles para usuarios sin formación jurídica?

Conversational Appropriateness:
¿En qué medida el sistema maneja conversaciones sobre temas sensibles relacionados con género, demostrando empatía, utilizando un lenguaje apropiado y manteniendo un tono de apoyo y no crítico durante toda la interacción?

Procedural Correctness:
¿Con qué precisión el sistema identifica y explica los procedimientos legales, plazos, requisitos documentales y vías institucionales específicas aplicables al caso dentro del sistema judicial colombiano?

Legal Utility:
¿En qué grado los consejos y recomendaciones proporcionados por el sistema tienen valor práctico para mejorar los resultados del usuario en el proceso legal, incluyendo la priorización efectiva de información crítica y pasos accionables?

for each question the system will be evaluated with a score from 1 to 5.



Moreover we will use the CUQ questionaire for actual layover people
when finishing the process in order to get feedback on the 
bot's more general performance. [citation!!!!]