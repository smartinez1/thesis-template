\chapter{State of the Art}
\label{cha:sota}

%%
% \section{Top level section}

% %%%%
% \subsection{Second level section}

% %%%%%%
% \subsubsection{Third level section} 

\section{State of the Art}

The rapid advancement of Large Language Models (LLMs) and their applications 
in legal contexts has opened new avenues for making legal systems more accessible, 
particularly for non-expert users. LLMs have demonstrated significant utility in processing 
legal documents, generating advice, and improving legal decision-making processes, 
yet challenges remain, particularly in highly specialized domains such as law. 
Recent works have shown that, while LLMs can assist laypeople with legal tasks, 
they also struggle with issues such as hallucination and the proper sourcing of 
high-quality legal data, as noted by Dahan et al. \cite{dahan2023lawyers}.

Legal chatbots, as highlighted by Chakrabortyi \cite{chakraborty2023revolutionizing}, 
have become powerful tools in democratizing access to legal services by offering 
cost-effective, on-demand guidance. These AI-driven systems, including well-known 
platforms such as DoNotPay and LegalZoom, help users draft documents and resolve 
disputes, although they cannot fully replace the nuanced expertise of human lawyers. 
Alongside these, works like Wu et al.'s 
Diagnostic Legal Large Language Model (D3LM) \cite{wu2024knowledgeinfusedlegalwisdomnavigating} 
focus on improving LLM interactions with non-expert users by employing lawyer-like diagnostic 
questions to better guide legal queries. This model's use of Positive-Unlabeled Reinforcement 
Learning (PURL) significantly enhances the user experience by generating more relevant, 
targeted questions, ensuring that pertinent legal factors are considered during interactions with the LLM.

In another vein, the NOMOS model by Pennisi et al. \cite{pennisi-etal-2023-nomos} 
focuses on the identification of legal obligations within statutes, addressing the 
complexity of legal language and enabling a more efficient extraction of actionable 
insights from dense legal texts. By combining Positional Embeddings (PE) with 
Temporal Convolutional Networks (TCNs), NOMOS demonstrates how deep learning can 
streamline the process of mining obligations from legal documents, 
making legal information more accessible.

Meanwhile, Jonathan Li et al. \cite{li2024experimentinglegalaisolutions} introduce a 
human-centric approach to legal question-answering systems in their work, which 
develops the LegalQA dataset. This dataset comprises real, expert-curated legal 
questions and answers spanning diverse legal areas. Their research emphasizes the 
use of structured data and retrieval-augmented generation to boost LLM performance, 
particularly in producing factually correct and comprehensible answers for laypeople. 
This framework is an important step towards addressing some of the gaps found in general-purpose LLMs, 
particularly by ensuring that the legal information delivered is both accurate and relevant to users' needs.

Mavi et al. \cite{mavi2023retrievalaugmentedchainofthoughtsemistructureddomains} 
explore the Retrieval-Augmented Chain-of-Thought (CoT) model, 
which tackles specialized legal tasks by efficiently retrieving 
context from semi-structured data. Their model enhances the reasoning abilities of LLMs, 
especially in legal question-answering scenarios, by incorporating important contextual 
information within token limitations.

In contrast to more complex methods like D3LM, Retrieval-Augmented Generation (RAG) 
approaches offer a compelling advantage by requiring less data and providing more 
straightforward processing pipelines. While D3LM utilizes diagnostic questions and 
relies on Positive-Unlabeled Reinforcement Learning (PURL) to guide legal interactions, 
its process is notably intricate, involving knowledge graphs that are highly specific to 
particular legal domains. This presents a limitation when the field of expertise changes, 
as the knowledge graph would need to be adjusted or rebuilt for different legal areas. 
RAG, on the other hand, bypasses this complexity by focusing on retrieving minimal, 
highly relevant text segments from large legal corpora, as demonstrated by 
LegalBench-RAG \cite{pipitone2024legalbenchragbenchmarkretrievalaugmentedgeneration}. 
By emphasizing precise retrieval and enabling LLMs to generate accurate citations, 
RAG methods allow for efficient, domain-agnostic solutions that maintain accuracy 
without the overhead of domain-specific modeling.

Moreover, while the Chain-of-Thought (CoT) approach, as proposed by Mavi et al. 
\cite{mavi2023retrievalaugmentedchainofthoughtsemistructureddomains}, offers valuable 
step-by-step reasoning that improves the decision-making process in complex legal tasks, 
it comes with its own challenges. CoT relies heavily on structured reasoning, 
which is well-suited for certain specialized legal tasks but can increase the complexity 
of model outputs. This added complexity, especially when dealing with semi-structured data, 
can sometimes hinder scalability and efficiency when compared to the more straightforward 
retrieval and generation process of RAG. Thus, while CoT has clear benefits in 
enhancing accuracy for complex legal queries, RAGâ€™s simplicity and adaptability 
make it particularly advantageous for real-world applications where legal fields 
and contexts are constantly shifting.

One of the most prominent problems when it comes to LLMs, especially in the legal domain, is the 
presence of hallucinations. For instance, when replying to a legal question, the model may come up
with non-existent laws or contradictory recomendations. It is of very high importance then, to avoid 
this kind of issues. In \cite{hu2025finetuninglargelanguagemodels} the authors create a benchmark
to evaluate model's hallucinations, using the groundtruth law set as a 

This combination of streamlined retrieval and adaptability makes RAG methods particularly 
suited to dynamic legal environments, such as Colombia's, where legal systems can be 
difficult for non-experts to navigate.

One of the recent developments in domain-specific LLMs for regulatory and financial text interpretation is our work \cite{martinez-etal-2025-scalable}, 
where we present a comprehensive approach to developing LLMs tailored specifically for understanding regulatory texts. 
Our methodology involves constructing a specialized corpus through large-scale scraping of financial and regulatory 
documents across domains like compliance, licensing, and financial reporting. 
By preprocessing this data with GPT-4o-mini and further pre-training a LLaMA-3.1-8B model on the curated corpus, 
we demonstrate improved capability in tasks such as acronym expansion and regulatory question-answering. 
Our implementation of Quantized Low-Rank Adaptation (QLoRA) addresses computational efficiency concerns while maintaining model performance. 
Although our model shows only slight improvements over baseline in some tasks, particularly in named entity recognition, 
this approach illustrates the potential of domain-specific LLMs in regulatory text interpretation and establishes a foundation for specialized NLP evaluation methodologies. 
This scalable framework offers a balanced alternative between computational requirements and performance, allowing potential deployment on more accessible hardware compared to larger models, 
making it particularly relevant for contexts where computational resources may be limited.
\endinput

